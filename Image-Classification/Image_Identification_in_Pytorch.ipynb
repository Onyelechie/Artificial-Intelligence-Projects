{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqKqn2Rs0iXw"
      },
      "source": [
        "# PyTorch Project\n",
        "**Goal**: Create a neural network (multi-layer perceptron) to classify hand-written digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRodqWN4096d"
      },
      "source": [
        "## Imports\n",
        "- `torch`: PyTorch\n",
        "- `torch.nn`: A part of PyTorch that deals specifically with neural network tools\n",
        "- `torch.nn.functional`: A collection of functions that are useful in machine learning\n",
        "- `torchvision.datasets`: Contains many common datasets for computer vision.\n",
        "- `torch.optim`: Contains various optimizers for training the NN.\n",
        "- `torchvision.transforms`: Contains useful \"transform\" layers that you can use to manipulate your data before applying any neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0cpeMvoNbph3"
      },
      "outputs": [],
      "source": [
        "## Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms as T\n",
        "\n",
        "## Optional, for visualization purposes.\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpM_VCVblqlB"
      },
      "source": [
        "## PyTorch Data Types\n",
        "\n",
        "All data in PyTorch is represented using a data type called a *Tensor*.  \n",
        "A tensor is a multi-dimensional array of data (entries in the array can be 32-bit floats, 64-bit floats, etc.)  \n",
        "Tensors in PyTorch are almost exactly the same as NdArrays in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ksNybzRWlpp1"
      },
      "outputs": [],
      "source": [
        "## Example tensor\n",
        "tensor = torch.tensor([[1,2],[3,4]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzyK7X_Ulxcj"
      },
      "source": [
        "## Representing Images as Tensors\n",
        "We want our neural network to process (black and white) images.  \n",
        "To feed these images into our neural network we first need to represent them as tensors, as all data must be represented as a tensor to be used in PyTorch.  \n",
        "  \n",
        "**Idea:** We can represent an image as a 2d grid, where the entries in the grid are the pixel values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MM6Q-thBX3Xj"
      },
      "outputs": [],
      "source": [
        "tensor = torch.ones((100,100))\n",
        "tensor[30:50,30:50] = 0\n",
        "# helpful function for showing an image\n",
        "def show_image(image):\n",
        "  plt.imshow(image, cmap='gray_r')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM1HiK3tl0eN",
        "outputId": "dc0ca33f-2af8-43fd-d9d4-ed714b6da8a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a basic image and visualize it\n",
        "tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfOxzEhy4GJN"
      },
      "source": [
        "# Hyperparameters\n",
        "Hyperparameters are parameters that you (the ML engineer) set in order to control the model.\n",
        "The hyperparameters we'll use are:\n",
        "- **Learning Rate**: Control how big the \"steps\" are during gradient descent\n",
        "- **Hidden Layer Units**: Controls how many hidden units are in the hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W8ljiOuV6EEN"
      },
      "outputs": [],
      "source": [
        "## Hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "HIDDEN_LAYER_UNITS = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPbSp7R5Pq8"
      },
      "source": [
        "## Importing the data\n",
        "We import the MNIST dataset from `torchvision.datasets`.\n",
        "\n",
        "### Transforming the data\n",
        "The data, as given, are actual images. In order to work with this in a neural network, we need to convert it to a tensor (array) of pixel values.\n",
        "\n",
        "Thus, we first apply a `ToTensor()` transform which transforms each images into a 1x28x28 array of pixel values (the 1 is because these are grayscale images. If these were RGB it would be 3x28x28).\n",
        "\n",
        "![img](https://umai.pro/transform_data.gif)\n",
        "\n",
        "### `DataLoader`\n",
        "PyTorch's `DataLoader` class acts as a wrapper around a `DataSet`, and allows you to seamlessly fetch data in batches, as well as shuffle the dataset to reduce overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "siWzEiIyiHq0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 28033754.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 20703417.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 12421205.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5512305.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Fetching data and transform\n",
        "\n",
        "train_dataset = datasets.MNIST('./data',train = True, download=True, transform = T.ToTensor())\n",
        "\n",
        "test_dataset = datasets.MNIST('./data',train = False, download=True, transform = T.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0qvjZin2mGZj",
        "outputId": "7ad9c9ee-2cf9-4fd2-bc62-6af5e8d21c30"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualize an image in our dataset\n",
        "show_image(train_dataset[12][0].squeeze())\n",
        "train_dataset[12][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRefzrmnMLE6"
      },
      "source": [
        "## Creating the NN\n",
        "Models in pytorch are defined as a class. We extend `torch.nn.Module` to include all of pytorch's powerful features.\n",
        "\n",
        "The structure is as follows:\n",
        "### `__init__` (constructor):\n",
        "Here we define the various 'layers' of our NN model. In this case we will have an input layer, 2 hidden layers, and an output layer. When we talk of 'layers', you can really think of them as separate, single-layer neural nets chained together.\n",
        "\n",
        "### `forward`\n",
        "Here, we defined what happens to the data as it goes through the network. This is broken down into the following steps:  \n",
        "1. `flatten` the 28x28 array into a 784x1 array.   \n",
        "2. Apply each layer in order, with a `relu` activation function after each.\n",
        "3. Convert whatever numbers the network outputs into a probability, assigning a higher probability for higher output values. This is called `softmax`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aRF0bs-mlepp"
      },
      "outputs": [],
      "source": [
        "# NN Module\n",
        "class SimpleNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(SimpleNN, self).__init__()\n",
        "    self.fc1 = nn.Linear(784,HIDDEN_LAYER_UNITS) #input layer\n",
        "    self.fc2 = nn.Linear(HIDDEN_LAYER_UNITS,HIDDEN_LAYER_UNITS) #hidden layer\n",
        "    self.fc3 = nn.Linear(HIDDEN_LAYER_UNITS,10) #output layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x , start_dim=1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    x = F.softmax(x, dim = 1)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq3owfx1Xbpz"
      },
      "source": [
        "## Creating An Instance of The Module\n",
        "Next, we use the module we created to create a single neural network. If we wanted to have multiple networks (with the same structure), we'd instantiate this multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Q4W-dBIznV5w"
      },
      "outputs": [],
      "source": [
        "model = SimpleNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX0HKZ1hMSJ7"
      },
      "source": [
        "## Defining the Optimizer and Loss Function\n",
        "The `optimizer` is what actually changes the weight values in the model. You pass in the model parameters that it will operate on, as well as a learning rate. The most basic optimizer is `SGD` (stochastic gradient descent) which operates using calculus. PyTorch offers a couple different optimizers (visualized below) that perform differently depending on your problem.\n",
        "<br></br>\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/1*47skUygd3tWf3yB9A10QHg.gif\" width=\"300\">\n",
        "\n",
        "  \n",
        "<br></br>\n",
        "We also define a loss function which we will use to evaluate model predictions. For this task, Negative Log Likelihood (`NLLLoss`) is appropriate.\n",
        "- **Idea**: encourage the model to assign a high probability to the correct label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bMOZlWsPMTDR"
      },
      "outputs": [],
      "source": [
        "# Create the optimizer and loss_func\n",
        "\n",
        "loss_func = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWibxw-lOLJz"
      },
      "source": [
        "## Training/Validation Loop\n",
        "Next, we loop over the training dataset (in batches), calculate the loss, the gradient (derivative), and adjust the weights.\n",
        "\n",
        "Then we loop over the validation (test) dataset, take the highest probability label as the model's prediction, and see how many predictions it gets right. We report this as the accuracy.\n",
        "\n",
        "We can run the above steps multiple times, each time we do so, it's known as a single 'epoch'. We continue doing this until the accuracy appears to level off.\n",
        "<br></br>\n",
        "<img src=\"https://umai.pro/training_cycle.gif\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sonYHxuxjEhm",
        "outputId": "0e4e09fa-5175-4e43-f52d-fe047d4487ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.5347)\n",
            "tensor(0.7626)\n",
            "tensor(0.8328)\n",
            "tensor(0.8659)\n",
            "tensor(0.8846)\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "for epoch in range(5):\n",
        "  for batch, (data, targets) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(data)\n",
        "    loss = loss_func(torch.log(out),targets)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#Testing loop\n",
        "  correct =0\n",
        "  for batch, (data, targets) in enumerate(test_loader):\n",
        "    out = model(data)\n",
        "    best_guesses = out.argmax(dim=1)\n",
        "    correct_now =(targets == best_guesses)\n",
        "    correct += correct_now.sum()\n",
        "  print(correct/len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caFoyutYmPkU"
      },
      "source": [
        "## Using our Model\n",
        "Now that we have trained a model, we can use it for handwritten digit recognition! For this demo we use a model we trained earlier today to save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SJnAlbm-mP4b",
        "outputId": "542fc072-2849-42c2-f56a-47d5eeabc05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# visualizing an image\n",
        "sample_image = train_dataset[12][0]\n",
        "out = model(sample_image)\n",
        "print(out.argmax(dim=1))\n",
        "show_image(sample_image.squeeze())\n",
        "#print(train_dataset[12][0])\n",
        "# show_image(sample_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BQ7L4OfqlxO"
      },
      "source": [
        "Our simple model (trained for 50 epochs) has over 93% accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpSHCHlfn-fC",
        "outputId": "bbd95710-2b30-4150-b317-c4bb7d4fd408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Correct: 0.8845999836921692\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "for batch, (data, targets) in enumerate(test_loader):\n",
        "    out = model(data)\n",
        "    best_guesses = out.argmax(dim=1) # this just takes the maximum probability label as the predicted label\n",
        "    correct += (best_guesses==targets).sum()\n",
        "\n",
        "print(f'Test Correct: {correct/len(test_dataset)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
